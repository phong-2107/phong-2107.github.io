[
{
	"uri": "/vi/2-prerequiste/2.1-createec2/",
	"title": "Chuẩn bị S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ cần tạo một S3 Bucket để lưu trữ các dữ liệu gốc (raw data) và dữ liệu đã xử lý (processed data). S3 Bucket sẽ là nơi Lambda function đọc và ghi dữ liệu trong quá trình pipeline xử lý file.\nSau khi tạo S3 Bucket, bạn sẽ cấu hình các quyền truy cập cho các dịch vụ AWS liên quan (ví dụ, AWS Lambda) để đảm bảo rằng các dịch vụ có thể truy cập và thao tác với dữ liệu trong S3.\nTổng quan về kiến trúc lưu trữ dữ liệu:\nS3 Raw Bucket: Lưu trữ dữ liệu gốc trước khi xử lý (ví dụ: log từ hệ thống). S3 Processed Bucket: Lưu trữ dữ liệu sau khi được xử lý và chuyển đổi định dạng. Để tìm hiểu cách tạo và cấu hình S3 Bucket, bạn có thể tham khảo tài liệu AWS sau:\nGiới thiệu về Amazon S3 Nội dung Tạo S3 Bucket Cấu hình quyền truy cập Tạo folder trong S3 "
},
{
	"uri": "/vi/3-accessibilitytoinstances/3.1-create-layer/",
	"title": "Create Lambda Layer",
	"tags": [],
	"description": "",
	"content": "Tạo Lambda Layer Trong bước này, chúng ta sẽ tạo một Lambda Layer để đóng gói các thư viện và dependencies cần thiết (ví dụ: pandas, pyarrow) cho Lambda function.\nCác bước thực hiện: Chuẩn bị gói ZIP\nThư mục gốc trong file ZIP cần có cấu trúc: python/lib/python3.11/site-packages/... ZIP này chứa các thư viện cần thiết (pandas, pyarrow). Ảnh minh họa cấu trúc thư mục trong file ZIP:\nTạo Lambda Layer trên AWS Console\nTruy cập AWS Console → Lambda → Layers → Create layer.\nName: layer-pandas-pyarrow. Upload file layer-x86_64.zip. Compatible runtimes: chọn Python 3.11. Click Create để hoàn tất.\n"
},
{
	"uri": "/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Workshop \u0026ldquo;Serverless File Processing with Amazon Athena và AWS Lambda\u0026rdquo; cung cấp một cái nhìn toàn diện về cách triển khai pipeline xử lý dữ liệu serverless sử dụng các dịch vụ của AWS như Amazon Athena và AWS Lambda. Trong workshop này, bạn sẽ học cách tối ưu hóa quy trình xử lý và phân tích dữ liệu với các công cụ mạnh mẽ mà không cần quản lý cơ sở hạ tầng phức tạp.\nBạn sẽ thực hành các bước quan trọng như:\nXử lý và chuyển đổi dữ liệu: Sử dụng AWS Lambda để chuyển đổi dữ liệu từ các định dạng như CSV/JSON sang Parquet, giúp tối ưu hóa việc lưu trữ và truy vấn. Trích xuất dữ liệu: Lambda cũng sẽ hỗ trợ việc trích xuất dữ liệu, chuẩn hóa và xử lý dữ liệu để có thể dễ dàng phân tích. Giám sát và quản lý: Cấu hình và sử dụng Amazon CloudWatch để theo dõi toàn bộ quy trình, đảm bảo mọi bước xử lý đều diễn ra suôn sẻ. Với Serverless Architecture, bạn không phải lo lắng về việc quản lý server, giảm thiểu chi phí và tối ưu hiệu suất. Hãy cùng chúng tôi thực hành và tìm hiểu cách xây dựng một hệ thống xử lý dữ liệu hiệu quả và tiết kiệm chi phí bằng cách sử dụng Amazon Athena và AWS Lambda.\nTrong workshop này, bạn sẽ học được những lợi ích chính sau:\nKhông cần quản lý server: Toàn bộ pipeline xử lý sẽ được chạy trên các dịch vụ serverless của AWS, giúp bạn tập trung vào việc xây dựng ứng dụng thay vì quản lý cơ sở hạ tầng. Tối ưu chi phí: Chỉ trả tiền cho những tài nguyên thực tế sử dụng, giúp tiết kiệm chi phí đáng kể so với các phương pháp truyền thống. Tăng hiệu suất: Các dịch vụ AWS như Athena giúp bạn truy vấn dữ liệu nhanh chóng mà không cần phải tải toàn bộ dữ liệu vào hệ thống, giảm bớt độ trễ. Dễ dàng mở rộng: Vì là serverless, hệ thống có thể dễ dàng mở rộng khi dữ liệu tăng trưởng mà không cần thay đổi cấu trúc hay thêm phần cứng. Hãy tham gia workshop này để khám phá cách AWS giúp bạn xây dựng ứng dụng serverless mạnh mẽ và tiết kiệm chi phí!\n"
},
{
	"uri": "/vi/2-prerequiste/2.1-createec2/2.1.1-createbucket/",
	"title": "Tạo S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Tạo S3 Bucket Trong bước này, chúng ta sẽ tạo hai S3 Buckets: một để lưu trữ dữ liệu gốc (Raw Data) và một để lưu trữ dữ liệu đã xử lý (Processed Data). Cấu trúc thư mục sẽ giúp chúng ta dễ dàng phân biệt và truy vấn dữ liệu sau này.\nTruy cập AWS S3 Console tại giao diện quản trị dịch vụ S3.\nClick Create bucket. Tạo Bucket 1: edu-logs-raw\nTại mục Bucket name, điền tên bucket theo định dạng: edu-logs-raw-. (Tên bucket phải là duy nhất trên toàn cầu) Tại mục Region, chọn khu vực Region gần bạn nhất. Tại mục Block Public Access, giữ mặc định là Keep enabled (để bảo mật). Tại mục Bucket Versioning, chọn Enable (khuyến nghị để giữ các phiên bản của dữ liệu). Click Create để tạo Bucket 1. Tạo Bucket 2: edu-logs-processed\nTạo bucket thứ hai với tên edu-logs-processed- theo cấu trúc tương tự. Duy trì các cài đặt giống như Bucket 1. Click Create để tạo Bucket 2. Lưu ý: Khi tạo xong các S3 Buckets, bạn sẽ cần cấu hình quyền truy cập cho Lambda function (thực hiện bước tiếp theo trong phần Cấu hình quyền truy cập) để Lambda có thể truy cập và ghi dữ liệu vào các Bucket này.\n"
},
{
	"uri": "/vi/",
	"title": "Serverless File Processing with Amazon Athena and AWS Lambda",
	"tags": [],
	"description": "",
	"content": "Serverless File Processing with Amazon Athena và AWS Lambda Tổng quan Trong workshop này, bạn sẽ tìm hiểu và thực hành cách xây dựng một pipeline serverless sử dụng Amazon Athena và AWS Lambda. Bạn sẽ học cách xử lý và phân tích dữ liệu file, bao gồm chuyển đổi định dạng, trích xuất dữ liệu và giám sát toàn bộ quy trình.\nDưới đây là kiến trúc mẫu của mô hình sử dụng serverless với Lambda và Athena: Nội dung Giới thiệu Các bước chuẩn bị Xử lý và chuyển đổi dữ liệu với AWS Lambda Tạo bảng và quét dữ liệu với AWS Glue Truy vấn dữ liệu với Amazon Athena Giám sát quy trình với Amazon CloudWatch Dọn dẹp tài nguyên "
},
{
	"uri": "/vi/2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Để chuẩn bị cho workshop \u0026ldquo;Serverless File Processing with Amazon Athena và AWS Lambda\u0026rdquo;, bạn cần thực hiện một số bước chuẩn bị cơ bản về môi trường AWS, bao gồm việc tạo S3 Bucket để lưu trữ dữ liệu và IAM Role để cấp quyền truy cập cho các dịch vụ AWS.\nTrong phần này, bạn sẽ thực hiện các bước chuẩn bị như:\nChuẩn bị S3 Bucket: Tạo S3 Bucket để lưu trữ dữ liệu gốc và dữ liệu đã xử lý từ pipeline Lambda. Tạo IAM Role: Tạo IAM Role cho Lambda để có thể truy cập và xử lý dữ liệu trong S3 Bucket cũng như sử dụng các dịch vụ AWS khác cần thiết cho pipeline. Nội dung Chuẩn bị S3 bucket Tạo IAM Role Tải tài nguyên "
},
{
	"uri": "/vi/2-prerequiste/2.1-createec2/2.1.2-setuppermissions/",
	"title": "Cấu hình quyền truy cập",
	"tags": [],
	"description": "",
	"content": "Cấu hình quyền truy cập cho S3 Bucket Trong bước này, chúng ta sẽ cấu hình IAM Role cho AWS Lambda để Lambda có thể truy cập và thao tác với dữ liệu trong S3 Buckets (edu-logs-raw và edu-logs-processed). Bằng cách sử dụng IAM Policy, chúng ta sẽ cấp quyền cho Lambda để thực hiện các tác vụ như đọc và ghi dữ liệu từ S3.\nTạo Policy với quyền truy cập cần thiết: Click Create policy ở tab mới. Chọn JSON và dán chính xác IAM Policy sau vào phần JSON. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadRaw\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-raw-*/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3WriteProcessed\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::edu-logs-processed-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LogsWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;,\u0026#34;logs:CreateLogStream\u0026#34;,\u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueMinimal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;,\u0026#34;glue:GetTable\u0026#34;,\u0026#34;glue:CreateTable\u0026#34;,\u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;,\u0026#34;glue:BatchCreatePartition\u0026#34;,\u0026#34;glue:BatchDeletePartition\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCustomMetrics\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudwatch:PutMetricData\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Đặt tên Policy name EduLogLambdaPolicy Kiểm tra các IAM Policy Nếu đã đầy đủ chọn Create policy Truy cập vào AWS IAM Console tại giao diện quản trị IAM.\nClick Roles ở menu bên trái. Click Create role. Tạo Role cho Lambda:\nStep 1: Chọn AWS service. Step 2: Chọn AWS service là Lambda trong phần Trusted entity. Step 3: Click Next để chuyển sang bước gán quyền. Gán policy cho role:\nChọn thanh tìm kiếm và tìm EduLogLambdaPolicy. Chọn EduLogLambdaPolicy và chọn Next. Gán policy cho role:\nĐặt tên cho role lambda-edu-log-role. Kiểm tra đã nhận Policy . Chọn Create role để hoàn thành bước này. "
},
{
	"uri": "/vi/2-prerequiste/2.2-createiamrole/",
	"title": "Cấu hình quyền truy cập",
	"tags": [],
	"description": "",
	"content": "Cấu hình quyền truy cập cho S3 Bucket Trong bước này, chúng ta sẽ cấu hình IAM Role cho AWS Lambda để Lambda có thể truy cập và thao tác với dữ liệu trong S3 Buckets (edu-logs-raw và edu-logs-processed). Bằng cách sử dụng IAM Policy, chúng ta sẽ cấp quyền cho Lambda để thực hiện các tác vụ như đọc và ghi dữ liệu từ S3.\nTạo Policy với quyền truy cập cần thiết: Click Create policy ở tab mới. Chọn JSON và dán chính xác IAM Policy sau vào phần JSON. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadRaw\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-raw-*/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3WriteProcessed\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::edu-logs-processed-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LogsWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;,\u0026#34;logs:CreateLogStream\u0026#34;,\u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueMinimal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;,\u0026#34;glue:GetTable\u0026#34;,\u0026#34;glue:CreateTable\u0026#34;,\u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;,\u0026#34;glue:BatchCreatePartition\u0026#34;,\u0026#34;glue:BatchDeletePartition\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCustomMetrics\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudwatch:PutMetricData\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Đặt tên Policy name EduLogLambdaPolicy Kiểm tra các IAM Policy Nếu đã đầy đủ chọn Create policy Truy cập vào AWS IAM Console tại giao diện quản trị IAM.\nClick Roles ở menu bên trái. Click Create role. Tạo Role cho Lambda:\nStep 1: Chọn AWS service. Step 2: Chọn AWS service là Lambda trong phần Trusted entity. Step 3: Click Next để chuyển sang bước gán quyền. Gán policy cho role:\nChọn thanh tìm kiếm và tìm EduLogLambdaPolicy. Chọn EduLogLambdaPolicy và chọn Next. Gán policy cho role:\nĐặt tên cho role lambda-edu-log-role. Kiểm tra đã nhận Policy . Chọn Create role để hoàn thành bước này. "
},
{
	"uri": "/vi/3-accessibilitytoinstances/3.2-create-lambda/",
	"title": "Tạo Lambda Function để xử lý dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tạo Lambda Function để xử lý dữ liệu Trong bước này, chúng ta sẽ tạo một Lambda Function có nhiệm vụ xử lý và chuyển đổi dữ liệu từ định dạng CSV hoặc JSON sang Parquet và lưu trữ dữ liệu đã xử lý vào S3 Processed Bucket.\nCác bước thực hiện: Truy cập AWS Lambda Console:\nMở AWS Lambda Console tại Lambda Console. Click Create function. Chọn cấu hình cho Lambda:\nAuthor from scratch. Function name: Nhập tên Lambda: process-edu-logs. Runtime: Chọn Python 3.11 Architecture: Chọn x86_64. Chọn Permissions cho Lambda:\nExecution role: Chọn Use an existing role . Tìm và chọn: lambda-edu-log-role. Click Create function. "
},
{
	"uri": "/vi/3-accessibilitytoinstances/3.3-config-lambda/",
	"title": "Cấu hình cho lambda",
	"tags": [],
	"description": "",
	"content": "Cấu hình cho lambda Trong bước này chúng ta sẽ cấu hình cho lambda, các kết nối và thiết lập cần thiết\nCác bước thực hiện: Add layer:\nMở Lambda fuction\ntrong phần Code Click chọn Layers.\nChọn Add a Layer. Chọn Custom layers.\nTìm layer-pandas-pyarrow với Version (latest).\nchọn Add. cấu hình Environment variables:\nChọn Configuration. Tìm Environment variables. chọn Edit. Nhập 2 key: . RAW_BUCKET = edu-logs-raw-\u0026lt;account\u0026gt;. PROCESSED_BUCKET = edu-logs-processed-\u0026lt;account\u0026gt;. chọn Save. . cấu hình basic settings:\nChọn Configuration. Tìm General configuration. chọn Edit. Memory = 2048 . Timeout = 2 min. chọn Save. Gắn S3 trigger cho Lambda: Tab Configuration → Triggers → Add trigger Source: S3. Bucket: Chọn edu-logs-raw. Event type: chọn All object create events. Prefix: ingest/ chọn Add. Viết mã Lambda Function: Sau khi tạo Lambda function, bạn sẽ cần thêm mã nguồn để xử lý dữ liệu: import boto3, os, io, json import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from datetime import timezone from dateutil import parser as dtparser s3 = boto3.client(\u0026#34;s3\u0026#34;) RAW_BUCKET = os.environ[\u0026#34;RAW_BUCKET\u0026#34;] PROCESSED_BUCKET = os.environ[\u0026#34;PROCESSED_BUCKET\u0026#34;] def _to_df(key: str, body: bytes) -\u0026gt; pd.DataFrame: if key.lower().endswith(\u0026#34;.json\u0026#34;): data = json.loads(body) if isinstance(data, dict): data = [data] return pd.json_normalize(data) if key.lower().endswith(\u0026#34;.csv\u0026#34;): return pd.read_csv(io.BytesIO(body)) raise ValueError(f\u0026#34;Unsupported format: {key}\u0026#34;) def _norm_cols(df: pd.DataFrame) -\u0026gt; pd.DataFrame: df.columns = [c.strip().lower().replace(\u0026#34; \u0026#34;, \u0026#34;_\u0026#34;) for c in df.columns] # chuẩn hóa timestamp for tcol in (\u0026#34;timestamp\u0026#34;,\u0026#34;event_time\u0026#34;,\u0026#34;time\u0026#34;): if tcol in df.columns: df[\u0026#34;timestamp\u0026#34;] = pd.to_datetime(df[tcol], errors=\u0026#34;coerce\u0026#34;, utc=True) break if \u0026#34;timestamp\u0026#34; not in df.columns: df[\u0026#34;timestamp\u0026#34;] = pd.Timestamp.utcnow().tz_localize(\u0026#34;UTC\u0026#34;) return df.drop_duplicates() def lambda_handler(event, _ctx): rec = event[\u0026#34;Records\u0026#34;][0] bucket = rec[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = rec[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if bucket != RAW_BUCKET: print(f\u0026#34;Skip other bucket: {bucket}\u0026#34;) return obj = s3.get_object(Bucket=bucket, Key=key) body = obj[\u0026#34;Body\u0026#34;].read() df = _norm_cols(_to_df(key, body)) ts = pd.to_datetime(df[\u0026#34;timestamp\u0026#34;].iloc[0], utc=True) y, m, d = ts.year, f\u0026#34;{ts.month:02d}\u0026#34;, f\u0026#34;{ts.day:02d}\u0026#34; table = pa.Table.from_pandas(df) buf = io.BytesIO() pq.write_table(table, buf, compression=\u0026#34;snappy\u0026#34;) out_key = f\u0026#34;year={y}/month={m}/day={d}/{os.path.splitext(os.path.basename(key))[0]}.parquet\u0026#34; s3.put_object(Bucket=PROCESSED_BUCKET, Key=out_key, Body=buf.getvalue()) print(f\u0026#34;Processed s3://{bucket}/{key} -\u0026gt; s3://{PROCESSED_BUCKET}/{out_key}\u0026#34;) Kiểm tra và deploy Lambda: Sau khi viết mã, click Deploy để triển khai Lambda function. Bạn có thể kiểm tra Lambda bằng cách sử dụng Test trong Lambda Console hoặc thông qua sự kiện S3 mà chúng ta sẽ cấu hình ở bước tiếp theo. "
},
{
	"uri": "/vi/2-prerequiste/2.3-download/",
	"title": "Tải tài nguyên",
	"tags": [],
	"description": "",
	"content": "Tải tài nguyên trong bước này chúng ta sẽ tải tài nguyên cần thiết cho workshop và data phù hợp\ntruy cập đường link: coppy đường link sau https://github.com/phong-2107/workshop-Data.git ở tab mới. sau đó tải về. Giải nén để sử dụng cho việc thực hành phía sau "
},
{
	"uri": "/vi/2-prerequiste/2.1-createec2/2.1.3-createfolders/",
	"title": "Tạo Thư Mục trong S3",
	"tags": [],
	"description": "",
	"content": "Tạo Thư Mục trong S3 Trong bước này, chúng ta sẽ tạo cấu trúc thư mục trong S3 Processed Bucket để tổ chức dữ liệu đã xử lý một cách hiệu quả. Mặc dù Lambda sẽ tự động phân chia dữ liệu theo thời gian (năm, tháng, ngày), nhưng chúng ta có thể tạo trước cấu trúc thư mục để dễ dàng theo dõi và tổ chức dữ liệu.\nCấu hình Prefix trong edu-logs-processed (tuỳ chọn):\nTrong Bucket edu-logs-processed, tạo cấu trúc thư mục (prefix) phân chia theo ngày tháng như sau: year=YYYY/month=MM/day=DD/ Ví dụ: year=2025/month=08/day=11/ Lưu ý: Không bắt buộc phải tạo các thư mục trước. Lambda sẽ tự động tạo và ghi dữ liệu vào đúng partition theo thời gian. Việc tạo cấu trúc thư mục này sẽ giúp chúng ta quản lý dữ liệu dễ dàng hơn và dễ dàng truy vấn khi sử dụng Athena.\nLưu ý: Cấu trúc thư mục này là tùy chọn vì Lambda sẽ tự động tạo các partition khi xử lý dữ liệu. Tuy nhiên, việc tạo sẵn cấu trúc thư mục có thể giúp bạn hình dung rõ ràng hơn về cách dữ liệu được lưu trữ và phân chia.\n"
},
{
	"uri": "/vi/3-accessibilitytoinstances/",
	"title": "Xử lý và chuyển đổi dữ liệu với AWS Lambda",
	"tags": [],
	"description": "",
	"content": "Xử lý và chuyển đổi dữ liệu với AWS Lambda Trong bước này, chúng ta sẽ sử dụng AWS Lambda để xử lý và chuyển đổi dữ liệu từ các định dạng như CSV hoặc JSON sang định dạng Parquet, tối ưu hóa việc lưu trữ và truy vấn dữ liệu trong S3. Lambda sẽ thực hiện các tác vụ xử lý dữ liệu tự động khi có dữ liệu mới được tải lên S3 Raw Bucket.\nLambda function sẽ thực hiện các tác vụ sau:\nĐọc dữ liệu từ S3 Raw Bucket. Chuyển đổi định dạng dữ liệu từ CSV/JSON sang Parquet. Lưu trữ dữ liệu đã xử lý vào S3 Processed Bucket. Phân chia dữ liệu theo ngày, tháng, năm để tối ưu hóa truy vấn và lưu trữ. Để thực hiện điều này, bạn cần:\nTạo một Lambda function có quyền truy cập vào cả S3 Raw Bucket và S3 Processed Bucket. Cấu hình Lambda để kích hoạt khi có sự kiện mới (mới file được tải lên S3). Nội dung 3.1. Tạo Lambda Layer để kết nối 3.2. Tạo Lambda Function để xử lý dữ liệu 3.3. Cấu hình sự kiện S3 để kích hoạt Lambda 3.4. Cập nhật dữ liệu lên S3 và kiểm tra "
},
{
	"uri": "/vi/4-glue-crawler/",
	"title": "Cấu hình AWS Glue Data Catalog và Crawler",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tạo Database trong AWS Glue Data Catalog để lưu metadata (schema) cho dữ liệu đã xử lý. Tạo Crawler quét S3 Processed Bucket và tự động tạo/cập nhật Table trong Glue. Sau bước này, dữ liệu Parquet trong edu-logs-processed-... sẽ có bảng để truy vấn bằng Amazon Athena. Yêu cầu trước khi làm bước này\nLambda đã chạy và ghi file Parquet vào s3://edu-logs-processed-\u0026lt;your-unique-name\u0026gt;/year=YYYY/month=MM/day=DD/... 1) Tạo Database trong Glue Data Catalog Vào AWS Console → mở AWS Glue\nBên trái chọn Data Catalog → Databases → Add database Điền: Database name: edu_logs_db Các mục khác giữ mặc định Nhấn Create database 2) Tạo Crawler để quét S3 Processed Trong AWS Glue, chọn Crawlers → Create crawler Name: nhập: crawler-edu-logs-processed Chon; Next để tiếp tục Data sources: Click Add a data source → chọn S3 Include path: nhập đường dẫn bucket processed: s3://edu-logs-processed-\u0026lt;your-unique-name\u0026gt;/ Nhấn Add data source 3) Chọn/IAM Role cho Crawler Ở bước Choose an IAM role: Chọn AWSGlueServiceRoleDefault Nhấn Next Đảm bảo role này có quyền đọc từ edu-logs-processed-... (mặc định wizard sẽ tạo đủ quyền đọc S3 cho crawler). 4) Cấu hình Output (Database, tiền tố bảng, lịch chạy) Output: Target database: chọn edu_logs_db Table name prefix (optional): điền logs_ để bảng tạo ra có tiền tố dễ nhận biết (ví dụ logs_edu_logs_processed) Frequency chon On demand Review → Create để tạo crawler Hình minh họa (sẽ cập nhật)\n5) Chạy Crawler lần đầu Quay lại danh sách Crawlers Chọn crawler-edu-logs-processed → nhấn Run crawler Chờ trạng thái chạy Completed (đợi vài chục giây đến vài phút) 6) Kiểm tra bảng \u0026amp; schema Vào AWS Glue → Data Catalog → Tables Bạn sẽ thấy bảng mới: Click vào bảng → tab Schema để xem các cột/kiểu dữ liệu có đúng kỳ vọng không\nKiểm tra tab Location của bảng trỏ về thư mục: "
},
{
	"uri": "/vi/3-accessibilitytoinstances/3.4-upload-file/",
	"title": "Upload file mẫu để kiểm tra Lambda",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Trong bước này, chúng ta sẽ upload dữ liệu mẫu vào S3 Raw Bucket để kích hoạt Lambda function xử lý dữ liệu, quan sát log trên CloudWatch và kiểm tra dữ liệu đã được xử lý trong S3 Processed Bucket.\nUpload dữ liệu mẫu vào S3:\nMở AWS S3 Console → chọn bucket edu-logs-raw-. Click Create folder → đặt tên folder là ingest/ → Create folder. Mở folder ingest/ vừa tạo → Upload → Upload 2 file dữ liệu mẫu:\nFile 1: edu_activity.json File 2: edu_activity.csv Click Upload để tải file lên bucket. Việc upload file vào folder ingest/ sẽ tự động kích hoạt Lambda function đã cấu hình ở bước trước. Quan sát log trên CloudWatch:\nMở ** AWS CloudWatch Console** → chọn Logs → Log groups. Chọn log group /aws/lambda/process-edu-logs. Mở log stream mới nhất để xem quá trình xử lý file. Xác nhận trong log có thông báo: Processed s3://edu-logs-raw... -\u0026gt; s3://edu-logs-processed.../\u0026lt;partition\u0026gt;/\u0026lt;file\u0026gt;.parquet ** Kiểm tra dữ liệu đầu ra trong S3 Processed Bucket**:\nMở bucket edu-logs-processed-. Duyệt theo cấu trúc: year=2025/month=08/day=08/. Xác nhận có file .parquet được tạo tương ứng với file dữ liệu mẫu vừa upload. "
},
{
	"uri": "/vi/5-athena/",
	"title": "Truy vấn dữ liệu với Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Cấu hình Amazon Athena để truy vấn dữ liệu đã xử lý trong S3 Processed Bucket. Thực hiện các truy vấn SQL mẫu để phân tích dữ liệu log. Cấu hình Partition Projection để tối ưu chi phí và tốc độ truy vấn. Mở Athena và cấu hình kết quả truy vấn\nVào AWS Console → chọn Athena → Query editor. Ở góc trên bên phải, click biểu tượng Settings → Manage. Tại Location of query result, nhập: s3://edu-logs-processed-\u0026lt;account\u0026gt;/athena-results/ Click Save để lưu cấu hình. Chọn Data source và Database\nData source: chọn AwsDataCatalog.\nDatabase: chọn edu_logs_db.\nTrong Tables, chọn bảng logs_edu_logs_processed để kiểm tra schema.\nChạy các truy vấn SQL mẫu\nXem 10 bản ghi đầu tiên:\nSELECT * FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; LIMIT 10; Top hoạt động theo người dùng:\nSELECT user_id, COUNT(*) AS activity_count FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; WHERE year = 2025 AND month = 8 GROUP BY user_id ORDER BY activity_count DESC; Thời lượng học trung bình theo khoá học:\nSELECT course_id, AVG(CAST(duration_sec AS DOUBLE)) AS avg_duration_sec FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; WHERE year = 2025 AND month = 8 GROUP BY course_id ORDER BY avg_duration_sec DESC; (Tuỳ chọn) Cấu hình Partition Projection để tối ưu truy vấn\nTrong Athena → Tables → chọn bảng logs_edu_logs_processed → Edit table. Thêm các Parameters sau: projection.enabled = true\rprojection.year.type = integer\rprojection.year.range = 2024,2030\rprojection.month.type = integer\rprojection.month.range = 1,12\rprojection.month.digits = 2\rprojection.day.type = integer\rprojection.day.range = 1,31\rprojection.day.digits = 2\rstorage.location.template = s3://edu-logs-processed-\u0026lt;account\u0026gt;/year=${year}/month=${month}/day=${day}/ Lưu thay đổi. Lưu ý: Khi sử dụng Partition Projection, trong truy vấn bắt buộc lọc theo year, month, day để hạn chế lượng dữ liệu quét và giảm chi phí.\nVí dụ:\nSELECT * FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed\u0026#34; WHERE year = 2025 AND month = 8 AND day = 8; "
},
{
	"uri": "/vi/6-cloudwatch/",
	"title": "Giám sát quy trình với Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Trong bước này, chúng ta sẽ:\nGiám sát log của Lambda Function bằng CloudWatch Logs Insights. Tạo Metric Filter để đếm số file được xử lý. Thiết lập SNS + CloudWatch Alarm để nhận cảnh báo khi có lỗi. Kiểm thử quy trình end-to-end. Sử dụng CloudWatch Logs Insights để truy vấn log Lambda Vào AWS Console → CloudWatch → Logs → Logs Insights. Tại Log groups, chọn /aws/lambda/process-edu-logs. Dán truy vấn mẫu sau: fields @timestamp, @message | filter @message like /Processed/ | sort @timestamp desc | limit 50 Chạy query để xem các log gần đây có chứa từ khóa Processed (báo hiệu file đã xử lý thành công). 2. Tạo Metric Filter để đếm số file xử lý Vào CloudWatch → Logs → Log groups → /aws/lambda/process-edu-logs. Chọn Metric filters → Create metric filter. Filter pattern: Processed Metric name: ProcessedFilesCount Namespace: Edu/LogPipeline Value: 1 Sau khi tạo xong, vào CloudWatch → Metrics → Edu/LogPipeline để thấy metric mới. 3. Tạo SNS + CloudWatch Alarm để cảnh báo lỗi Lambda Bước 1: Tạo SNS Topic Vào Simple Notification Service (SNS) → Create topic. Type: Standard Name: edu-pipeline-alerts Create subscription: Protocol: Email Endpoint: Nhập email của bạn. Mở email và Confirm subscription. Bước 2: Tạo CloudWatch Alarm Vào CloudWatch → Alarms → Create alarm. Chọn metric: Lambda → By function name → process-edu-logs → Errors. Điều kiện: \u0026gt;= 3 trong 5 phút. Notification: Chọn topic edu-pipeline-alerts. Create alarm. 4. Kiểm thử quy trình End-to-End Upload thêm file vào: s3://edu-logs-raw-\u0026lt;account\u0026gt;/ingest/\nVào CloudWatch Logs để xem log của Lambda cho lần chạy mới.\nKiểm tra S3 processed bucket có file .parquet trong đúng prefix: year=YYYY/month=MM/day=DD/\nChạy Crawler (nếu không dùng Partition Projection) → Query lại Athena.\nXác nhận:\nMetric ProcessedFilesCount tăng.\nAlarm không đỏ (trừ khi bạn đang test lỗi).\n"
},
{
	"uri": "/vi/7-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Giải phóng các tài nguyên AWS đã tạo trong workshop để tránh phát sinh chi phí. Xóa các bucket, role, hàm Lambda, và các cấu hình liên quan. 1. Xóa dữ liệu và Bucket S3 Vào AWS S3 Console. Xóa toàn bộ dữ liệu trong: edu-logs-raw-\u0026lt;account\u0026gt; edu-logs-processed-\u0026lt;account\u0026gt; Sau khi trống dữ liệu, chọn Empty và Delete bucket cho cả hai bucket. 2. Xóa Lambda Function Vào AWS Lambda Console. Chọn hàm Lambda process-edu-logs → Actions → Delete. 3. Xóa IAM Role và Policy Vào IAM Console. Chọn Roles → tìm lambda-edu-log-role → Delete role. Vào Policies → tìm EduLogLambdaPolicy → Delete policy. 4. Xóa Glue Crawler và Database Vào AWS Glue Console. Chọn Crawlers → chọn crawler-edu-logs-processed → Delete. Chọn Databases → chọn edu_logs_db → Delete. 5. Xóa Workgroup và Kết quả Athena Vào Amazon Athena → Workgroups → xóa workgroup tùy chỉnh (nếu có). Xóa thư mục athena-results/ trong bucket edu-logs-processed-\u0026lt;account\u0026gt;. 6. Xóa Metric và Alarm trên CloudWatch Vào CloudWatch Console: Alarms → xóa alarm liên quan. Log groups → /aws/lambda/process-edu-logs → Delete log group. Metrics → xóa metric custom trong namespace Edu/LogPipeline. 7. Xóa SNS Topic Vào SNS Console → chọn edu-pipeline-alerts → Delete topic. 8. Xác nhận không còn tài nguyên hoạt động Kiểm tra lại: S3 không còn bucket workshop. Lambda không còn function process-edu-logs. IAM không còn role/policy liên quan. Glue không còn database/crawler. CloudWatch không còn log group, metric, alarm. SNS không còn topic. "
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]