[
{
	"uri": "/2-prerequiste/2.1-createec2/2.1.1-createbucket/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Create S3 Buckets In this step, we will create two S3 Buckets: one for storing the raw data (Raw Data) and one for storing the processed data (Processed Data).\nThis folder structure will make it easier to distinguish and query data later.\nAccess the AWS S3 Console via the S3 Management Console.\nClick Create bucket.\nCreate Bucket 1: edu-logs-raw\nIn Bucket name, enter the bucket name in the format: edu-logs-raw-. (Bucket names must be globally unique.) In Region, select the region closest to you. In Block Public Access, keep the default setting Keep enabled (for security purposes).\nIn Bucket Versioning, select Enable (recommended to maintain data version history). Click Create to create Bucket 1.\nCreate Bucket 2: edu-logs-processed\nCreate the second bucket with the name edu-logs-processed-, following the same structure. Keep the same settings as Bucket 1. Click Create to create Bucket 2. Note: After creating the S3 Buckets, you will need to configure access permissions for the Lambda function (see the next step in Set Up Permissions) so that Lambda can read from and write to these buckets.\n"
},
{
	"uri": "/2-prerequiste/2.1-createec2/",
	"title": "Prepare S3 Bucket",
	"tags": [],
	"description": "",
	"content": "In this step, we will need to create an S3 Bucket to store both raw data and processed data. The S3 Bucket will serve as the location where the Lambda function reads and writes data during the file processing pipeline.\nAfter creating the S3 Bucket, you will configure the necessary access permissions for the relevant AWS services (e.g., AWS Lambda) to ensure that these services can access and manipulate the data stored in S3.\nOverview of the data storage architecture:\nS3 Raw Bucket: Stores raw data before processing (e.g., logs from the system). S3 Processed Bucket: Stores data after it has been processed and converted. To learn how to create and configure an S3 Bucket, you can refer to the AWS documentation:\nIntroduction to Amazon S3 Content Create S3 Bucket Configure Access Permissions Create Folders in S3 "
},
{
	"uri": "/3-accessibilitytoinstances/3.1-create-layer/",
	"title": "Tạo Lambda Function để xử lý dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tạo Lambda layer Trong bước này, chúng ta sẽ tạo một Lambda layer có nhiệm vụ đóng gói thư viện, dependencies cần thiết cho Lambda function.\nCác bước thực hiện: Truy cập AWS Lambda Console:\nMở AWS Lambda Console tại Lambda Console. Chọn Layers ở menu bên trái Click Create function. Chọn cấu hình cho Lambda:\nAuthor from scratch. Function name: Nhập tên Lambda, ví dụ: process-file-lambda. Runtime: Chọn Python 3.11 Role: Chọn Create a new role with basic Lambda permissions. Gán quyền truy cập cho Lambda:\nLambda sẽ cần quyền truy cập vào S3 để đọc và ghi dữ liệu. Vì vậy, bạn sẽ gán IAM Role cho Lambda: Attach policies: Chọn AWSLambdaBasicExecutionRole để cấp quyền ghi log vào CloudWatch. Create custom policy: Chọn S3ReadWritePolicy để Lambda có quyền truy cập vào S3 Raw Bucket và S3 Processed Bucket. Bạn có thể tạo một policy mới như sau: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-processed-*\u0026#34; ] } ] } Viết mã Lambda Function: Sau khi tạo Lambda function, bạn sẽ cần thêm mã nguồn để xử lý dữ liệu: import json import boto3 import pandas as pd import pyarrow.parquet as pq import pyarrow as pa from io import BytesIO def lambda_handler(event, context): # Lấy thông tin về bucket và file từ sự kiện S3 bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Kết nối với S3 s3 = boto3.client(\u0026#39;s3\u0026#39;) # Tải dữ liệu từ S3 Raw Bucket response = s3.get_object(Bucket=bucket, Key=key) file_content = response[\u0026#39;Body\u0026#39;].read() # Đọc dữ liệu CSV/JSON if key.endswith(\u0026#39;.csv\u0026#39;): df = pd.read_csv(BytesIO(file_content)) elif key.endswith(\u0026#39;.json\u0026#39;): df = pd.read_json(BytesIO(file_content)) # Chuyển đổi dữ liệu sang Parquet table = pa.Table.from_pandas(df) buffer = BytesIO() pq.write_table(table, buffer) # Lưu dữ liệu Parquet vào S3 Processed Bucket processed_bucket = \u0026#39;edu-logs-processed-youraccount\u0026#39; processed_key = key.replace(\u0026#34;raw\u0026#34;, \u0026#34;processed\u0026#34;).replace(\u0026#34;.csv\u0026#34;, \u0026#34;.parquet\u0026#34;).replace(\u0026#34;.json\u0026#34;, \u0026#34;.parquet\u0026#34;) s3.put_object(Bucket=processed_bucket, Key=processed_key, Body=buffer.getvalue()) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;File {key} processed and saved as {processed_key}\u0026#39;) } Kiểm tra và deploy Lambda: Sau khi viết mã, click Deploy để triển khai Lambda function. Bạn có thể kiểm tra Lambda bằng cách sử dụng Test trong Lambda Console hoặc thông qua sự kiện S3 mà chúng ta sẽ cấu hình ở bước tiếp theo. "
},
{
	"uri": "/1-introduce/",
	"title": "Workshop Introduction",
	"tags": [],
	"description": "",
	"content": "\u0026ldquo;Serverless File Processing with Amazon Athena and AWS Lambda\u0026rdquo; workshop provides a comprehensive overview of how to implement a serverless data pipeline using AWS services such as Amazon Athena and AWS Lambda. In this workshop, you will learn how to optimize the data processing and analysis workflow with powerful tools, without the need to manage complex infrastructure.\nYou will practice key steps such as:\nData Processing and Conversion: Using AWS Lambda to convert data from formats like CSV/JSON to Parquet, which optimizes storage and querying. Data Extraction: Lambda will also assist with data extraction, normalization, and processing, ensuring that the data is ready for analysis. Monitoring and Management: Configuring and utilizing Amazon CloudWatch to monitor the entire workflow, ensuring that each processing step runs smoothly. With a Serverless Architecture, there is no need to worry about managing servers, reducing costs, and optimizing performance. Join us to practice and explore how to build an efficient and cost-effective data processing system using Amazon Athena and AWS Lambda.\nIn this workshop, you will learn the following key benefits:\nNo Server Management Required: The entire processing pipeline runs on AWS serverless services, allowing you to focus on building applications rather than managing infrastructure. Cost Optimization: Pay only for the resources you actually use, resulting in significant savings compared to traditional methods. Improved Performance: AWS services like Athena enable fast data querying without needing to load the entire dataset into the system, reducing latency. Scalability: As a serverless solution, the system can scale easily as data grows without requiring changes to the structure or additional hardware. Join this workshop to discover how AWS helps you build powerful and cost-effective serverless applications!\n"
},
{
	"uri": "/",
	"title": "Serverless File Processing with Amazon Athena and AWS Lambda",
	"tags": [],
	"description": "",
	"content": "Serverless File Processing with Amazon Athena and AWS Lambda Overview In this workshop, you will learn and practice how to build a serverless pipeline using Amazon Athena and AWS Lambda.\nYou will explore how to process and analyze file-based data, including format conversion, data extraction, and end-to-end workflow monitoring.\nBelow is a sample architecture diagram for a serverless model leveraging AWS Lambda and Amazon Athena:\nWorkshop Content Introduction Preparation Steps Data Processing and Transformation with AWS Lambda Creating Tables and Crawling Data with AWS Glue Querying Data with Amazon Athena Monitoring the Workflow with Amazon CloudWatch Cleaning Up Resources "
},
{
	"uri": "/2-prerequiste/2.1-createec2/2.1.2-setuppermissions/",
	"title": "Configure Access Permissions",
	"tags": [],
	"description": "",
	"content": "Configure Access Permissions for S3 Bucket In this step, we will configure an IAM Role for AWS Lambda to allow Lambda to access and manipulate data in the S3 Buckets (edu-logs-raw and edu-logs-processed). By using IAM Policy, we will grant Lambda the necessary permissions to perform actions such as reading and writing data from S3.\nAccess the AWS IAM Console at the IAM Console.\nClick on Roles in the left-hand menu. Click Create role. Create Role for Lambda:\nStep 1: Select AWS service as Lambda under Trusted entity. Step 2: Click Next to move to the permissions step. Create Policy with Necessary Permissions:\nClick Create policy in the new tab. Select JSON and paste the following IAM Policy into the JSON editor. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadRaw\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-raw-*/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3WriteProcessed\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::edu-logs-processed-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LogsWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueMinimal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:BatchCreatePartition\u0026#34;, \u0026#34;glue:BatchDeletePartition\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCustomMetrics\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudwatch:PutMetricData\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Set the Policy name to EduLogLambdaPolicy. Review the IAM Policy settings. If everything is correct, click Create policy.\nAccess the AWS IAM Console at the IAM Management Console.\nClick Roles in the left-hand menu. Click Create role.\nCreate a Role for Lambda:\nStep 1: Select AWS service. Step 2: Under Trusted entity type, choose Lambda. Step 3: Click Next to proceed to the permission assignment step.\nAttach the policy to the role:\nUse the search bar to find EduLogLambdaPolicy. Select EduLogLambdaPolicy and click Next.\nName and create the role:\nSet the Role name to lambda-edu-log-role. Verify that the role has the correct policy attached. Click Create role to complete the process.\n"
},
{
	"uri": "/2-prerequiste/2.2-createiamrole/",
	"title": "Configure Access Permissions",
	"tags": [],
	"description": "",
	"content": "Configure Access Permissions for S3 Bucket In this step, we will configure an IAM Role for AWS Lambda to allow Lambda to access and manipulate data in the S3 Buckets (edu-logs-raw and edu-logs-processed). By using IAM Policy, we will grant Lambda the necessary permissions to perform actions such as reading and writing data from S3.\nAccess the AWS IAM Console at the IAM Console.\nClick on Roles in the left-hand menu. Click Create role. Create Role for Lambda:\nStep 1: Select AWS service as Lambda under Trusted entity. Step 2: Click Next to move to the permissions step. Create Policy with Necessary Permissions:\nClick Create policy in the new tab. Select JSON and paste the following IAM Policy into the JSON editor. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadRaw\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-raw-*/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3WriteProcessed\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::edu-logs-processed-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LogsWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueMinimal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:BatchCreatePartition\u0026#34;, \u0026#34;glue:BatchDeletePartition\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCustomMetrics\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudwatch:PutMetricData\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Set the Policy name to EduLogLambdaPolicy. Review the IAM Policy configuration. If everything is correct, click Create policy.\nAccess the AWS IAM Console via the IAM Management Console.\nClick Roles from the left-hand menu. Click Create role.\nCreate a Role for Lambda:\nStep 1: Select AWS service. Step 2: Under Trusted entity type, choose Lambda. Step 3: Click Next to proceed to the permissions step.\nAttach the policy to the role:\nIn the search bar, type EduLogLambdaPolicy. Select EduLogLambdaPolicy and click Next.\nName and create the role:\nSet the Role name to lambda-edu-log-role. Confirm that the policy is attached to the role. Click Create role to complete the process.\n"
},
{
	"uri": "/3-accessibilitytoinstances/3.2-create-lambda/",
	"title": "Create a Lambda Function for Data Processing",
	"tags": [],
	"description": "",
	"content": "Create a Lambda Function for Data Processing In this step, we will create a Lambda Function that processes and converts data from CSV or JSON format into Parquet, then stores the processed data in the S3 Processed Bucket.\nSteps to follow: Access the AWS Lambda Console:\nOpen the AWS Lambda Console at Lambda Console. Click Create function.\nConfigure the Lambda function:\nAuthor from scratch. Function name: Enter the Lambda name: process-edu-logs. Runtime: Select Python 3.11. Architecture: Select x86_64.\nSet Permissions for Lambda:\nExecution role: Select Use an existing role. Search for and select: lambda-edu-log-role. Click Create function.\n"
},
{
	"uri": "/2-prerequiste/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "To prepare for the \u0026ldquo;Serverless File Processing with Amazon Athena and AWS Lambda\u0026rdquo; workshop, there are a few foundational steps to set up the AWS environment, including creating an S3 Bucket for data storage and an IAM Role to grant the necessary permissions for accessing services.\nIn this section, you will carry out the following preparations:\nSet up the S3 Bucket: Create an S3 Bucket to store both raw and processed data that will be handled by the Lambda function. Create IAM Role: Create an IAM Role for Lambda to access and process data in the S3 Bucket, as well as to use other AWS services required for the pipeline. Content Prepare S3 bucket Create IAM Role Download Resources "
},
{
	"uri": "/3-accessibilitytoinstances/3.3-config-lambda/",
	"title": "Configure Lambda",
	"tags": [],
	"description": "",
	"content": "Configure Lambda In this step, we will configure the Lambda function, including necessary connections and settings.\nSteps to follow: Add a layer:\nOpen the Lambda function.\nIn the Code section, click Layers.\nSelect Add a layer.\nChoose Custom layers.\nSearch for layer-pandas-pyarrow with the latest version.\nClick Add.\nConfigure environment variables:\nGo to Configuration. Find Environment variables. Click Edit.\nAdd the following two keys: RAW_BUCKET = edu-logs-raw-\u0026lt;account\u0026gt; PROCESSED_BUCKET = edu-logs-processed-\u0026lt;account\u0026gt; Click Save.\nConfigure basic settings:\nGo to Configuration. Find General configuration. Click Edit. Set Memory to 2048. Set Timeout to 2 minutes. Click Save.\nAttach an S3 trigger to Lambda:\nGo to Configuration → Triggers → Add trigger. Source: S3. Bucket: Select edu-logs-raw. Event type: Choose All object create events. Prefix: ingest/ Click Add.\nWrite the Lambda function code:\nAfter creating the Lambda function, add the following source code to process data: import boto3, os, io, json import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from datetime import timezone from dateutil import parser as dtparser s3 = boto3.client(\u0026#34;s3\u0026#34;) RAW_BUCKET = os.environ[\u0026#34;RAW_BUCKET\u0026#34;] PROCESSED_BUCKET = os.environ[\u0026#34;PROCESSED_BUCKET\u0026#34;] def _to_df(key: str, body: bytes) -\u0026gt; pd.DataFrame: if key.lower().endswith(\u0026#34;.json\u0026#34;): data = json.loads(body) if isinstance(data, dict): data = [data] return pd.json_normalize(data) if key.lower().endswith(\u0026#34;.csv\u0026#34;): return pd.read_csv(io.BytesIO(body)) raise ValueError(f\u0026#34;Unsupported format: {key}\u0026#34;) def _norm_cols(df: pd.DataFrame) -\u0026gt; pd.DataFrame: df.columns = [c.strip().lower().replace(\u0026#34; \u0026#34;, \u0026#34;_\u0026#34;) for c in df.columns] # Normalize timestamp for tcol in (\u0026#34;timestamp\u0026#34;, \u0026#34;event_time\u0026#34;, \u0026#34;time\u0026#34;): if tcol in df.columns: df[\u0026#34;timestamp\u0026#34;] = pd.to_datetime(df[tcol], errors=\u0026#34;coerce\u0026#34;, utc=True) break if \u0026#34;timestamp\u0026#34; not in df.columns: df[\u0026#34;timestamp\u0026#34;] = pd.Timestamp.utcnow().tz_localize(\u0026#34;UTC\u0026#34;) return df.drop_duplicates() def lambda_handler(event, _ctx): rec = event[\u0026#34;Records\u0026#34;][0] bucket = rec[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = rec[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if bucket != RAW_BUCKET: print(f\u0026#34;Skip other bucket: {bucket}\u0026#34;) return obj = s3.get_object(Bucket=bucket, Key=key) body = obj[\u0026#34;Body\u0026#34;].read() df = _norm_cols(_to_df(key, body)) ts = pd.to_datetime(df[\u0026#34;timestamp\u0026#34;].iloc[0], utc=True) y, m, d = ts.year, f\u0026#34;{ts.month:02d}\u0026#34;, f\u0026#34;{ts.day:02d}\u0026#34; table = pa.Table.from_pandas(df) buf = io.BytesIO() pq.write_table(table, buf, compression=\u0026#34;snappy\u0026#34;) out_key = f\u0026#34;year={y}/month={m}/day={d}/{os.path.splitext(os.path.basename(key))[0]}.parquet\u0026#34; s3.put_object(Bucket=PROCESSED_BUCKET, Key=out_key, Body=buf.getvalue()) print(f\u0026#34;Processed s3://{bucket}/{key} -\u0026gt; s3://{PROCESSED_BUCKET}/{out_key}\u0026#34;) Test and deploy the Lambda: After writing the code, click Deploy to publish the Lambda function. You can test it using the Test feature in the Lambda Console or by triggering it through an S3 event configured in the previous step. "
},
{
	"uri": "/2-prerequiste/2.1-createec2/2.1.3-createfolders/",
	"title": "Create Folders in S3",
	"tags": [],
	"description": "",
	"content": "Create Folders in S3 In this step, we will create the folder structure within the S3 Processed Bucket to organize the processed data efficiently. While Lambda will automatically handle the partitioning of data based on timestamps (year, month, day), we can pre-create the folder structure to make it easier to understand and visually organize the data.\nAccess the S3 Console at S3 Management Console.\nClick on the edu-logs-processed bucket. Click Create folder. Create Folder Structure:\nIn the Folder name field, input the partition structure in the format: year=YYYY/month=MM/day=DD/. For example: year=2025/month=08/day=11/. Click Create folder to organize the data. This will help in managing the data and make it easier to query later when using Athena.\nNote: The folder structure is optional as Lambda will create the partitions dynamically. However, creating it manually in advance can help you visualize the data storage and partitioning.\n"
},
{
	"uri": "/3-accessibilitytoinstances/",
	"title": "Data Processing and Transformation with AWS Lambda",
	"tags": [],
	"description": "",
	"content": "Data Processing and Transformation with AWS Lambda In this step, we will use AWS Lambda to process and transform data from formats such as CSV or JSON into the Parquet format, optimizing storage and query performance in Amazon S3.\nLambda will automatically perform data processing tasks whenever new data is uploaded to the S3 Raw Bucket.\nThe Lambda function will perform the following tasks:\nRead data from the S3 Raw Bucket. Convert the data format from CSV/JSON to Parquet. Store the processed data in the S3 Processed Bucket. Partition the data by day, month, and year to optimize queries and storage. To accomplish this, you need to:\nCreate a Lambda function with access permissions for both the S3 Raw Bucket and the S3 Processed Bucket. Configure Lambda to be triggered by new events (files uploaded to S3). Content 3.1. Create a Lambda Layer for Connectivity\n3.2. Create a Lambda Function for Data Processing\n3.3. Configure S3 Events to Trigger Lambda\n3.4. Upload Data to S3 and Verify\n"
},
{
	"uri": "/2-prerequiste/2.3-download/",
	"title": "Download Resources",
	"tags": [],
	"description": "",
	"content": "Download Resources In this step, we will download the necessary resources and datasets for the workshop.\nAccess the link: Copy the following link and open it in a new browser tab:\nhttps://github.com/phong-2107/workshop-Data.git Download the repository.\nExtract the downloaded files to prepare for the upcoming practice steps. "
},
{
	"uri": "/4-glue-crawler/",
	"title": "Configure AWS Glue Data Catalog and Crawler",
	"tags": [],
	"description": "",
	"content": "Objective Create a Database in the AWS Glue Data Catalog to store metadata (schema) for the processed data. Create a Crawler to scan the S3 Processed Bucket and automatically create/update a Table in Glue. After this step, the Parquet data in edu-logs-processed-... will be available as a table for querying with Amazon Athena. Prerequisites\nThe Lambda function has already run and written Parquet files to: s3://edu-logs-processed-\u0026lt;your-unique-name\u0026gt;/year=YYYY/month=MM/day=DD/... 1) Create a Database in the Glue Data Catalog In the AWS Console, open AWS Glue.\nOn the left menu, select Data Catalog → Databases → Add database.\nEnter: Database name: edu_logs_db Keep other settings as default. Click Create database.\n2) Create a Crawler to Scan the S3 Processed Bucket In AWS Glue, go to Crawlers → Create crawler.\nName: Enter crawler-edu-logs-processed.\nClick Next to continue.\nData sources:\nClick Add a data source → choose S3. Include path: enter the processed bucket path: s3://edu-logs-processed-\u0026lt;your-unique-name\u0026gt;/ Click Add data source.\n3) Choose an IAM Role for the Crawler In the Choose an IAM role step:\nSelect AWSGlueServiceRoleDefault. Click Next.\nEnsure this role has read permissions for edu-logs-processed-... (the wizard will typically create the necessary S3 read permissions by default).\n4) Configure Output (Database, Table Prefix, Schedule) Output: Target database: Select edu_logs_db. Table name prefix (optional): Enter logs_ so that generated tables have a recognizable prefix (e.g., logs_edu_logs_processed). Frequency: Select On demand.\nReview → Create to create the crawler.\n5) Run the Crawler for the First Time Go back to the Crawlers list. Select crawler-edu-logs-processed → click Run crawler. Wait until the status changes to Completed (this may take several seconds to a few minutes).\n6) Verify the Table \u0026amp; Schema Go to AWS Glue → Data Catalog → Tables. You should see a newly created table:\nClick the table → go to the Schema tab to review column names and data types.\nCheck the Location tab to ensure the table points to the correct folder path:\n"
},
{
	"uri": "/3-accessibilitytoinstances/3.4-upload-file/",
	"title": "Upload Sample Files to Test Lambda",
	"tags": [],
	"description": "",
	"content": "Objective In this step, we will upload sample data to the S3 Raw Bucket to trigger the Lambda function, monitor logs in CloudWatch, and verify the processed data in the S3 Processed Bucket.\nUpload sample data to S3:\nOpen the AWS S3 Console → select the bucket edu-logs-raw-. Click Create folder → name the folder ingest/ → click Create folder.\nOpen the newly created ingest/ folder → click Upload → upload two sample data files:\nFile 1: edu_activity.json\nFile 2: edu_activity.csv\nClick Upload to add the files to the bucket.\nUploading files into the ingest/ folder will automatically trigger the Lambda function configured in the previous step.\nMonitor logs in CloudWatch:\nOpen the AWS CloudWatch Console → select Logs → Log groups. Select the log group /aws/lambda/process-edu-logs.\nOpen the latest log stream to view the file processing logs. Confirm that the logs contain a message similar to:\nProcessed s3://edu-logs-raw... -\u0026gt; s3://edu-logs-processed.../\u0026lt;partition\u0026gt;/\u0026lt;file\u0026gt;.parquet\nVerify output data in the S3 Processed Bucket:\nOpen the bucket edu-logs-processed-. Navigate through the folder structure: year=2025/month=08/day=08/. Confirm that a .parquet file has been created corresponding to the uploaded sample data file.\n"
},
{
	"uri": "/5-athena/",
	"title": "Query Data with Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Objective Configure Amazon Athena to query processed data stored in the S3 Processed Bucket. Run sample SQL queries to analyze log data. Configure Partition Projection to optimize query cost and performance. Open Athena and Configure Query Results Location\nIn the AWS Console, go to Athena → Query editor.\nIn the top right corner, click the Settings icon → Manage.\nUnder Location of query result, enter: s3://edu-logs-processed-\u0026lt;account\u0026gt;/athena-results/ Click Save to store the configuration.\nSelect Data Source and Database\nData source: Select AwsDataCatalog. Database: Select edu_logs_db. Under Tables, choose the table logs_edu_logs_processed to review its schema.\nRun Sample SQL Queries\nView the first 10 records:\nSELECT * FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; LIMIT 10; Top activities by user:\nSELECT user_id, COUNT(*) AS activity_count FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; WHERE year = 2025 AND month = 8 GROUP BY user_id ORDER BY activity_count DESC; Average learning duration by course:\nSELECT course_id, AVG(CAST(duration_sec AS DOUBLE)) AS avg_duration_sec FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; WHERE year = 2025 AND month = 8 GROUP BY course_id ORDER BY avg_duration_sec DESC; (Optional) Configure Partition Projection for Query Optimization\nIn Athena → Tables → select the table logs_edu_logs_processed → Edit table. Add the following Parameters: projection.enabled = true\rprojection.year.type = integer\rprojection.year.range = 2024,2030\rprojection.month.type = integer\rprojection.month.range = 1,12\rprojection.month.digits = 2\rprojection.day.type = integer\rprojection.day.range = 1,31\rprojection.day.digits = 2\rstorage.location.template = s3://edu-logs-processed-\u0026lt;account\u0026gt;/year=${year}/month=${month}/day=${day}/ Save the changes. Note: When using Partition Projection, queries must filter by year, month, and day to reduce the amount of data scanned and lower costs.\nExample:\nSELECT * FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed\u0026#34; WHERE year = 2025 AND month = 8 AND day = 8; "
},
{
	"uri": "/6-cloudwatch/",
	"title": "Monitor the Workflow with Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "Objective In this step, we will:\nMonitor Lambda function logs using CloudWatch Logs Insights. Create a Metric Filter to count the number of processed files. Set up SNS + CloudWatch Alarm to receive error alerts. Test the end-to-end workflow. 1. Use CloudWatch Logs Insights to Query Lambda Logs In the AWS Console, go to CloudWatch → Logs → Logs Insights. Under Log groups, select /aws/lambda/process-edu-logs. Paste the following sample query: fields @timestamp, @message | filter @message like /Processed/ | sort @timestamp desc | limit 50 Run the query to view recent logs containing the keyword Processed (indicating the file was successfully processed). 2. Create a Metric Filter to Count Processed Files Go to CloudWatch → Logs → Log groups → /aws/lambda/process-edu-logs. Select Metric filters → Create metric filter. Filter pattern: Processed Metric name: ProcessedFilesCount Namespace: Edu/LogPipeline Value: 1 Once created, go to CloudWatch → Metrics → Edu/LogPipeline to see the new metric. 3. Create SNS + CloudWatch Alarm for Lambda Errors Step 1: Create an SNS Topic Go to Simple Notification Service (SNS) → Create topic. Type: Standard Name: edu-pipeline-alerts Create subscription: Protocol: Email Endpoint: Enter your email address. Open your email and Confirm subscription. Step 2: Create a CloudWatch Alarm Go to CloudWatch → Alarms → Create alarm. Select the metric: Lambda → By function name → process-edu-logs → Errors. Set the condition: \u0026gt;= 3 within 5 minutes. Notification: Choose the topic edu-pipeline-alerts. Click Create alarm. 4. Test the End-to-End Workflow Upload another file to:\ns3://edu-logs-raw-\u0026lt;account\u0026gt;/ingest/\nIn CloudWatch Logs, check the Lambda logs for the new execution.\nVerify the S3 Processed Bucket contains a .parquet file in the correct prefix:\nyear=YYYY/month=MM/day=DD/\nRun the Crawler (if not using Partition Projection) → Re-run queries in Athena.\nConfirm:\nThe ProcessedFilesCount metric has increased. The alarm status remains OK (unless you are testing an error scenario). "
},
{
	"uri": "/7-cleanup/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "Objective Release all AWS resources created during the workshop to avoid incurring ongoing costs. Delete buckets, roles, Lambda functions, and related configurations. 1. Delete Data and S3 Buckets Go to the AWS S3 Console. Delete all objects from: edu-logs-raw-\u0026lt;account\u0026gt; edu-logs-processed-\u0026lt;account\u0026gt; Once empty, select Empty and then Delete bucket for both buckets. 2. Delete the Lambda Function Go to the AWS Lambda Console. Select the Lambda function process-edu-logs → Actions → Delete. 3. Delete the IAM Role and Policy Go to the IAM Console. Select Roles → search for lambda-edu-log-role → Delete role. Go to Policies → search for EduLogLambdaPolicy → Delete policy. 4. Delete the Glue Crawler and Database Go to the AWS Glue Console. Select Crawlers → choose crawler-edu-logs-processed → Delete. Select Databases → choose edu_logs_db → Delete. 5. Delete Athena Workgroup and Query Results Go to Amazon Athena → Workgroups → delete any custom workgroup (if created). Delete the athena-results/ folder in the bucket edu-logs-processed-\u0026lt;account\u0026gt;. 6. Delete Metrics and Alarms in CloudWatch Go to the CloudWatch Console: Alarms → delete related alarms. Log groups → /aws/lambda/process-edu-logs → Delete log group. Metrics → delete the custom metric in the Edu/LogPipeline namespace. 7. Delete the SNS Topic Go to the SNS Console → select edu-pipeline-alerts → Delete topic. 8. Verify No Active Resources Remain Double-check: S3: No workshop buckets remain. Lambda: No process-edu-logs function. IAM: No related roles or policies. Glue: No database or crawler. CloudWatch: No log groups, metrics, or alarms. SNS: No topics. "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]