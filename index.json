[
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.1-createbucket/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Create S3 Bucket In this step, we will create two S3 Buckets: one to store raw data (Raw Data) and the other to store processed data (Processed Data). The folder structure will help us easily differentiate and query the data later.\nAccess AWS S3 Console at S3 Management Console.\nClick Create bucket. Create Bucket 1: edu-logs-raw\nIn the Bucket name field, enter the bucket name in the format: edu-logs-raw-. (Bucket names must be globally unique). In the Region section, choose the Region closest to you. For Block Public Access, keep the default setting as Keep enabled (for security). In the Bucket Versioning section, select Enable (recommended to keep versions of your data). Click Create to create Bucket 1. Create Bucket 2: edu-logs-processed\nCreate the second bucket with the name edu-logs-processed- following the same structure. Keep the settings the same as Bucket 1. Click Create to create Bucket 2. Configure Prefix in edu-logs-processed (optional):\nIn Bucket edu-logs-processed, create a folder structure (prefix) partitioned by year, month, and day as follows: year=YYYY/month=MM/day=DD/ Example: year=2025/month=08/day=11/ Note: It\u0026rsquo;s not mandatory to create these folders beforehand. Lambda will automatically create and write the data into the correct partition based on the timestamp. Note: After creating the S3 Buckets, you will need to configure access permissions for the Lambda function (this will be covered in the next step in Configure Access Permissions) to allow Lambda to access and write data to these buckets.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/",
	"title": "Prepare S3 Bucket",
	"tags": [],
	"description": "",
	"content": "In this step, we will need to create an S3 Bucket to store both raw data and processed data. The S3 Bucket will serve as the location where the Lambda function reads and writes data during the file processing pipeline.\nAfter creating the S3 Bucket, you will configure the necessary access permissions for the relevant AWS services (e.g., AWS Lambda) to ensure that these services can access and manipulate the data stored in S3.\nOverview of the data storage architecture:\nS3 Raw Bucket: Stores raw data before processing (e.g., logs from the system). S3 Processed Bucket: Stores data after it has been processed and converted. To learn how to create and configure an S3 Bucket, you can refer to the AWS documentation:\nIntroduction to Amazon S3 Content Create S3 Bucket Configure Access Permissions Create Folders in S3 "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.1-create-layer/",
	"title": "Tạo Lambda Function để xử lý dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tạo Lambda layer Trong bước này, chúng ta sẽ tạo một Lambda layer có nhiệm vụ đóng gói thư viện, dependencies cần thiết cho Lambda function.\nCác bước thực hiện: Truy cập AWS Lambda Console:\nMở AWS Lambda Console tại Lambda Console. Chọn Layers ở menu bên trái Click Create function. Chọn cấu hình cho Lambda:\nAuthor from scratch. Function name: Nhập tên Lambda, ví dụ: process-file-lambda. Runtime: Chọn Python 3.11 Role: Chọn Create a new role with basic Lambda permissions. Gán quyền truy cập cho Lambda:\nLambda sẽ cần quyền truy cập vào S3 để đọc và ghi dữ liệu. Vì vậy, bạn sẽ gán IAM Role cho Lambda: Attach policies: Chọn AWSLambdaBasicExecutionRole để cấp quyền ghi log vào CloudWatch. Create custom policy: Chọn S3ReadWritePolicy để Lambda có quyền truy cập vào S3 Raw Bucket và S3 Processed Bucket. Bạn có thể tạo một policy mới như sau: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-processed-*\u0026#34; ] } ] } Viết mã Lambda Function: Sau khi tạo Lambda function, bạn sẽ cần thêm mã nguồn để xử lý dữ liệu: import json import boto3 import pandas as pd import pyarrow.parquet as pq import pyarrow as pa from io import BytesIO def lambda_handler(event, context): # Lấy thông tin về bucket và file từ sự kiện S3 bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Kết nối với S3 s3 = boto3.client(\u0026#39;s3\u0026#39;) # Tải dữ liệu từ S3 Raw Bucket response = s3.get_object(Bucket=bucket, Key=key) file_content = response[\u0026#39;Body\u0026#39;].read() # Đọc dữ liệu CSV/JSON if key.endswith(\u0026#39;.csv\u0026#39;): df = pd.read_csv(BytesIO(file_content)) elif key.endswith(\u0026#39;.json\u0026#39;): df = pd.read_json(BytesIO(file_content)) # Chuyển đổi dữ liệu sang Parquet table = pa.Table.from_pandas(df) buffer = BytesIO() pq.write_table(table, buffer) # Lưu dữ liệu Parquet vào S3 Processed Bucket processed_bucket = \u0026#39;edu-logs-processed-youraccount\u0026#39; processed_key = key.replace(\u0026#34;raw\u0026#34;, \u0026#34;processed\u0026#34;).replace(\u0026#34;.csv\u0026#34;, \u0026#34;.parquet\u0026#34;).replace(\u0026#34;.json\u0026#34;, \u0026#34;.parquet\u0026#34;) s3.put_object(Bucket=processed_bucket, Key=processed_key, Body=buffer.getvalue()) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;File {key} processed and saved as {processed_key}\u0026#39;) } Kiểm tra và deploy Lambda: Sau khi viết mã, click Deploy để triển khai Lambda function. Bạn có thể kiểm tra Lambda bằng cách sử dụng Test trong Lambda Console hoặc thông qua sự kiện S3 mà chúng ta sẽ cấu hình ở bước tiếp theo. "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Workshop Introduction",
	"tags": [],
	"description": "",
	"content": "\u0026ldquo;Serverless File Processing with Amazon Athena and AWS Lambda\u0026rdquo; workshop provides a comprehensive overview of how to implement a serverless data pipeline using AWS services such as Amazon Athena and AWS Lambda. In this workshop, you will learn how to optimize the data processing and analysis workflow with powerful tools, without the need to manage complex infrastructure.\nYou will practice key steps such as:\nData Processing and Conversion: Using AWS Lambda to convert data from formats like CSV/JSON to Parquet, which optimizes storage and querying. Data Extraction: Lambda will also assist with data extraction, normalization, and processing, ensuring that the data is ready for analysis. Monitoring and Management: Configuring and utilizing Amazon CloudWatch to monitor the entire workflow, ensuring that each processing step runs smoothly. With a Serverless Architecture, there is no need to worry about managing servers, reducing costs, and optimizing performance. Join us to practice and explore how to build an efficient and cost-effective data processing system using Amazon Athena and AWS Lambda.\nIn this workshop, you will learn the following key benefits:\nNo Server Management Required: The entire processing pipeline runs on AWS serverless services, allowing you to focus on building applications rather than managing infrastructure. Cost Optimization: Pay only for the resources you actually use, resulting in significant savings compared to traditional methods. Improved Performance: AWS services like Athena enable fast data querying without needing to load the entire dataset into the system, reducing latency. Scalability: As a serverless solution, the system can scale easily as data grows without requiring changes to the structure or additional hardware. Join this workshop to discover how AWS helps you build powerful and cost-effective serverless applications!\n"
},
{
	"uri": "//localhost:1313/",
	"title": "Session Management",
	"tags": [],
	"description": "",
	"content": "Serverless File Processing with Amazon Athena và AWS Lambda Tổng quan Trong workshop này, bạn sẽ tìm hiểu và thực hành cách xây dựng một pipeline serverless sử dụng Amazon Athena và AWS Lambda. Bạn sẽ học cách xử lý và phân tích dữ liệu file, bao gồm chuyển đổi định dạng, trích xuất dữ liệu và giám sát toàn bộ quy trình.\nDưới đây là kiến trúc mẫu của mô hình sử dụng serverless với Lambda và Athena: Nội dung Giới thiệu Các bước chuẩn bị Xử lý và chuyển đổi dữ liệu với AWS Lambda Tạo bảng và quét dữ liệu với AWS Glue Truy vấn dữ liệu với Amazon Athena Giám sát quy trình với Amazon CloudWatch Dọn dẹp tài nguyên "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.2-setuppermissions/",
	"title": "Configure Access Permissions",
	"tags": [],
	"description": "",
	"content": "Configure Access Permissions for S3 Bucket In this step, we will configure an IAM Role for AWS Lambda to allow Lambda to access and manipulate data in the S3 Buckets (edu-logs-raw and edu-logs-processed). By using IAM Policy, we will grant Lambda the necessary permissions to perform actions such as reading and writing data from S3.\nAccess the AWS IAM Console at the IAM Console.\nClick on Roles in the left-hand menu. Click Create role. Create Role for Lambda:\nStep 1: Select AWS service as Lambda under Trusted entity. Step 2: Click Next to move to the permissions step. Create Policy with Necessary Permissions:\nClick Create policy in the new tab. Select JSON and paste the following IAM Policy into the JSON editor. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadRaw\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-raw-*/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3WriteProcessed\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::edu-logs-processed-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LogsWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueMinimal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:BatchCreatePartition\u0026#34;, \u0026#34;glue:BatchDeletePartition\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCustomMetrics\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudwatch:PutMetricData\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-createiamrole/",
	"title": "Configure Access Permissions",
	"tags": [],
	"description": "",
	"content": "Configure Access Permissions for S3 Bucket In this step, we will configure an IAM Role for AWS Lambda to allow Lambda to access and manipulate data in the S3 Buckets (edu-logs-raw and edu-logs-processed). By using IAM Policy, we will grant Lambda the necessary permissions to perform actions such as reading and writing data from S3.\nAccess the AWS IAM Console at the IAM Console.\nClick on Roles in the left-hand menu. Click Create role. Create Role for Lambda:\nStep 1: Select AWS service as Lambda under Trusted entity. Step 2: Click Next to move to the permissions step. Create Policy with Necessary Permissions:\nClick Create policy in the new tab. Select JSON and paste the following IAM Policy into the JSON editor. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadRaw\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-raw-*/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3WriteProcessed\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::edu-logs-processed-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LogsWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueMinimal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:BatchCreatePartition\u0026#34;, \u0026#34;glue:BatchDeletePartition\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCustomMetrics\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudwatch:PutMetricData\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "To prepare for the \u0026ldquo;Serverless File Processing with Amazon Athena and AWS Lambda\u0026rdquo; workshop, there are a few foundational steps to set up the AWS environment, including creating an S3 Bucket for data storage and an IAM Role to grant the necessary permissions for accessing services.\nIn this section, you will carry out the following preparations:\nSet up the S3 Bucket: Create an S3 Bucket to store both raw and processed data that will be handled by the Lambda function. Create IAM Role: Create an IAM Role for Lambda to access and process data in the S3 Bucket, as well as to use other AWS services required for the pipeline. Content Prepare S3 bucket Create IAM Role "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.2-create-lambda/",
	"title": "Tạo Lambda Function để xử lý dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tạo Lambda Function để xử lý dữ liệu Trong bước này, chúng ta sẽ tạo một Lambda Function có nhiệm vụ xử lý và chuyển đổi dữ liệu từ định dạng CSV hoặc JSON sang Parquet và lưu trữ dữ liệu đã xử lý vào S3 Processed Bucket.\nCác bước thực hiện: Truy cập AWS Lambda Console:\nMở AWS Lambda Console tại Lambda Console. Click Create function. Chọn cấu hình cho Lambda:\nAuthor from scratch. Function name: Nhập tên Lambda, ví dụ: process-file-lambda. Runtime: Chọn Python 3.11 Role: Chọn Create a new role with basic Lambda permissions. Gán quyền truy cập cho Lambda:\nLambda sẽ cần quyền truy cập vào S3 để đọc và ghi dữ liệu. Vì vậy, bạn sẽ gán IAM Role cho Lambda: Attach policies: Chọn AWSLambdaBasicExecutionRole để cấp quyền ghi log vào CloudWatch. Create custom policy: Chọn S3ReadWritePolicy để Lambda có quyền truy cập vào S3 Raw Bucket và S3 Processed Bucket. Bạn có thể tạo một policy mới như sau: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-processed-*\u0026#34; ] } ] } Viết mã Lambda Function: Sau khi tạo Lambda function, bạn sẽ cần thêm mã nguồn để xử lý dữ liệu: import json import boto3 import pandas as pd import pyarrow.parquet as pq import pyarrow as pa from io import BytesIO def lambda_handler(event, context): # Lấy thông tin về bucket và file từ sự kiện S3 bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Kết nối với S3 s3 = boto3.client(\u0026#39;s3\u0026#39;) # Tải dữ liệu từ S3 Raw Bucket response = s3.get_object(Bucket=bucket, Key=key) file_content = response[\u0026#39;Body\u0026#39;].read() # Đọc dữ liệu CSV/JSON if key.endswith(\u0026#39;.csv\u0026#39;): df = pd.read_csv(BytesIO(file_content)) elif key.endswith(\u0026#39;.json\u0026#39;): df = pd.read_json(BytesIO(file_content)) # Chuyển đổi dữ liệu sang Parquet table = pa.Table.from_pandas(df) buffer = BytesIO() pq.write_table(table, buffer) # Lưu dữ liệu Parquet vào S3 Processed Bucket processed_bucket = \u0026#39;edu-logs-processed-youraccount\u0026#39; processed_key = key.replace(\u0026#34;raw\u0026#34;, \u0026#34;processed\u0026#34;).replace(\u0026#34;.csv\u0026#34;, \u0026#34;.parquet\u0026#34;).replace(\u0026#34;.json\u0026#34;, \u0026#34;.parquet\u0026#34;) s3.put_object(Bucket=processed_bucket, Key=processed_key, Body=buffer.getvalue()) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;File {key} processed and saved as {processed_key}\u0026#39;) } Kiểm tra và deploy Lambda: Sau khi viết mã, click Deploy để triển khai Lambda function. Bạn có thể kiểm tra Lambda bằng cách sử dụng Test trong Lambda Console hoặc thông qua sự kiện S3 mà chúng ta sẽ cấu hình ở bước tiếp theo. "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.3-createfolders/",
	"title": "Create Folders in S3",
	"tags": [],
	"description": "",
	"content": "Create Folders in S3 In this step, we will create the folder structure within the S3 Processed Bucket to organize the processed data efficiently. While Lambda will automatically handle the partitioning of data based on timestamps (year, month, day), we can pre-create the folder structure to make it easier to understand and visually organize the data.\nAccess the S3 Console at S3 Management Console.\nClick on the edu-logs-processed bucket. Click Create folder. Create Folder Structure:\nIn the Folder name field, input the partition structure in the format: year=YYYY/month=MM/day=DD/. For example: year=2025/month=08/day=11/. Click Create folder to organize the data. This will help in managing the data and make it easier to query later when using Athena.\nNote: The folder structure is optional as Lambda will create the partitions dynamically. However, creating it manually in advance can help you visualize the data storage and partitioning.\n"
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.3-config-lambda/",
	"title": "Tạo Lambda Function để xử lý dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tạo Lambda Function để xử lý dữ liệu Trong bước này, chúng ta sẽ tạo một Lambda Function có nhiệm vụ xử lý và chuyển đổi dữ liệu từ định dạng CSV hoặc JSON sang Parquet và lưu trữ dữ liệu đã xử lý vào S3 Processed Bucket.\nCác bước thực hiện: Truy cập AWS Lambda Console:\nMở AWS Lambda Console tại Lambda Console. Click Create function. Chọn cấu hình cho Lambda:\nAuthor from scratch. Function name: Nhập tên Lambda, ví dụ: process-file-lambda. Runtime: Chọn Python 3.11 Role: Chọn Create a new role with basic Lambda permissions. Gán quyền truy cập cho Lambda:\nLambda sẽ cần quyền truy cập vào S3 để đọc và ghi dữ liệu. Vì vậy, bạn sẽ gán IAM Role cho Lambda: Attach policies: Chọn AWSLambdaBasicExecutionRole để cấp quyền ghi log vào CloudWatch. Create custom policy: Chọn S3ReadWritePolicy để Lambda có quyền truy cập vào S3 Raw Bucket và S3 Processed Bucket. Bạn có thể tạo một policy mới như sau: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-processed-*\u0026#34; ] } ] } Viết mã Lambda Function: Sau khi tạo Lambda function, bạn sẽ cần thêm mã nguồn để xử lý dữ liệu: import json import boto3 import pandas as pd import pyarrow.parquet as pq import pyarrow as pa from io import BytesIO def lambda_handler(event, context): # Lấy thông tin về bucket và file từ sự kiện S3 bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Kết nối với S3 s3 = boto3.client(\u0026#39;s3\u0026#39;) # Tải dữ liệu từ S3 Raw Bucket response = s3.get_object(Bucket=bucket, Key=key) file_content = response[\u0026#39;Body\u0026#39;].read() # Đọc dữ liệu CSV/JSON if key.endswith(\u0026#39;.csv\u0026#39;): df = pd.read_csv(BytesIO(file_content)) elif key.endswith(\u0026#39;.json\u0026#39;): df = pd.read_json(BytesIO(file_content)) # Chuyển đổi dữ liệu sang Parquet table = pa.Table.from_pandas(df) buffer = BytesIO() pq.write_table(table, buffer) # Lưu dữ liệu Parquet vào S3 Processed Bucket processed_bucket = \u0026#39;edu-logs-processed-youraccount\u0026#39; processed_key = key.replace(\u0026#34;raw\u0026#34;, \u0026#34;processed\u0026#34;).replace(\u0026#34;.csv\u0026#34;, \u0026#34;.parquet\u0026#34;).replace(\u0026#34;.json\u0026#34;, \u0026#34;.parquet\u0026#34;) s3.put_object(Bucket=processed_bucket, Key=processed_key, Body=buffer.getvalue()) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;File {key} processed and saved as {processed_key}\u0026#39;) } Kiểm tra và deploy Lambda: Sau khi viết mã, click Deploy để triển khai Lambda function. Bạn có thể kiểm tra Lambda bằng cách sử dụng Test trong Lambda Console hoặc thông qua sự kiện S3 mà chúng ta sẽ cấu hình ở bước tiếp theo. "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.4-upload-file/",
	"title": "Tạo Lambda Function để xử lý dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tạo Lambda Function để xử lý dữ liệu Trong bước này, chúng ta sẽ tạo một Lambda Function có nhiệm vụ xử lý và chuyển đổi dữ liệu từ định dạng CSV hoặc JSON sang Parquet và lưu trữ dữ liệu đã xử lý vào S3 Processed Bucket.\nCác bước thực hiện: Truy cập AWS Lambda Console:\nMở AWS Lambda Console tại Lambda Console. Click Create function. Chọn cấu hình cho Lambda:\nAuthor from scratch. Function name: Nhập tên Lambda, ví dụ: process-file-lambda. Runtime: Chọn Python 3.11 Role: Chọn Create a new role with basic Lambda permissions. Gán quyền truy cập cho Lambda:\nLambda sẽ cần quyền truy cập vào S3 để đọc và ghi dữ liệu. Vì vậy, bạn sẽ gán IAM Role cho Lambda: Attach policies: Chọn AWSLambdaBasicExecutionRole để cấp quyền ghi log vào CloudWatch. Create custom policy: Chọn S3ReadWritePolicy để Lambda có quyền truy cập vào S3 Raw Bucket và S3 Processed Bucket. Bạn có thể tạo một policy mới như sau: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::edu-logs-raw-*\u0026#34;, \u0026#34;arn:aws:s3:::edu-logs-processed-*\u0026#34; ] } ] } Viết mã Lambda Function: Sau khi tạo Lambda function, bạn sẽ cần thêm mã nguồn để xử lý dữ liệu: import json import boto3 import pandas as pd import pyarrow.parquet as pq import pyarrow as pa from io import BytesIO def lambda_handler(event, context): # Lấy thông tin về bucket và file từ sự kiện S3 bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Kết nối với S3 s3 = boto3.client(\u0026#39;s3\u0026#39;) # Tải dữ liệu từ S3 Raw Bucket response = s3.get_object(Bucket=bucket, Key=key) file_content = response[\u0026#39;Body\u0026#39;].read() # Đọc dữ liệu CSV/JSON if key.endswith(\u0026#39;.csv\u0026#39;): df = pd.read_csv(BytesIO(file_content)) elif key.endswith(\u0026#39;.json\u0026#39;): df = pd.read_json(BytesIO(file_content)) # Chuyển đổi dữ liệu sang Parquet table = pa.Table.from_pandas(df) buffer = BytesIO() pq.write_table(table, buffer) # Lưu dữ liệu Parquet vào S3 Processed Bucket processed_bucket = \u0026#39;edu-logs-processed-youraccount\u0026#39; processed_key = key.replace(\u0026#34;raw\u0026#34;, \u0026#34;processed\u0026#34;).replace(\u0026#34;.csv\u0026#34;, \u0026#34;.parquet\u0026#34;).replace(\u0026#34;.json\u0026#34;, \u0026#34;.parquet\u0026#34;) s3.put_object(Bucket=processed_bucket, Key=processed_key, Body=buffer.getvalue()) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;File {key} processed and saved as {processed_key}\u0026#39;) } Kiểm tra và deploy Lambda: Sau khi viết mã, click Deploy để triển khai Lambda function. Bạn có thể kiểm tra Lambda bằng cách sử dụng Test trong Lambda Console hoặc thông qua sự kiện S3 mà chúng ta sẽ cấu hình ở bước tiếp theo. "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/",
	"title": "Xử lý và chuyển đổi dữ liệu với AWS Lambda",
	"tags": [],
	"description": "",
	"content": "Xử lý và chuyển đổi dữ liệu với AWS Lambda Trong bước này, chúng ta sẽ sử dụng AWS Lambda để xử lý và chuyển đổi dữ liệu từ các định dạng như CSV hoặc JSON sang định dạng Parquet, tối ưu hóa việc lưu trữ và truy vấn dữ liệu trong S3. Lambda sẽ thực hiện các tác vụ xử lý dữ liệu tự động khi có dữ liệu mới được tải lên S3 Raw Bucket.\nLambda function sẽ thực hiện các tác vụ sau:\nĐọc dữ liệu từ S3 Raw Bucket. Chuyển đổi định dạng dữ liệu từ CSV/JSON sang Parquet. Lưu trữ dữ liệu đã xử lý vào S3 Processed Bucket. Phân chia dữ liệu theo ngày, tháng, năm để tối ưu hóa truy vấn và lưu trữ. Để thực hiện điều này, bạn cần:\nTạo một Lambda function có quyền truy cập vào cả S3 Raw Bucket và S3 Processed Bucket. Cấu hình Lambda để kích hoạt khi có sự kiện mới (mới file được tải lên S3). Nội dung 3.1. Tạo Lambda Layer để kết nối 3.2. Tạo Lambda Function để xử lý dữ liệu 3.3. Cấu hình sự kiện S3 để kích hoạt Lambda 3.4. Cập nhật dữ liệu lên S3 và kiểm tra\n"
},
{
	"uri": "//localhost:1313/4-glue-crawler/",
	"title": "Cấu hình AWS Glue Data Catalog và Crawler",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tạo Database trong AWS Glue Data Catalog để lưu metadata (schema) cho dữ liệu đã xử lý. Tạo Crawler quét S3 Processed Bucket và tự động tạo/cập nhật Table trong Glue. Sau bước này, dữ liệu Parquet trong edu-logs-processed-... sẽ có bảng để truy vấn bằng Amazon Athena. Yêu cầu trước khi làm bước này\nLambda đã chạy và ghi file Parquet vào s3://edu-logs-processed-\u0026lt;your-unique-name\u0026gt;/year=YYYY/month=MM/day=DD/... Bạn đã biết chính xác tên processed bucket của mình. 1) Tạo Database trong Glue Data Catalog Vào AWS Console → mở AWS Glue Bên trái chọn Data Catalog → Databases → Create database Điền: Database name: edu_logs_db Các mục khác giữ mặc định Nhấn Create database Hình minh họa (sẽ cập nhật)\n2) Tạo Crawler để quét S3 Processed Trong AWS Glue, chọn Crawlers → Create crawler\nName: nhập tên dễ nhớ, ví dụ:\nData sources:\nClick Add a data source → chọn S3 Include path: nhập đường dẫn bucket processed của bạn, ví dụ: s3://edu-logs-processed-\u0026lt;your-unique-name\u0026gt;/ Nhấn Add data source Crawler source properties (tuỳ giao diện): Recrawl behavior: Lần đầu: chọn Recrawl all files (Full scan toàn bộ) Về sau có thể đổi sang Recrawl changed partitions only để nhanh \u0026amp; rẻ hơn Hình minh họa (sẽ cập nhật)\n3) Chọn/IAM Role cho Crawler Ở bước Choose an IAM role: Chọn Create new IAM role hoặc dùng role có sẵn Ví dụ đặt tên role: AWSGlueServiceRole-edu Đảm bảo role này có quyền đọc từ edu-logs-processed-... (mặc định wizard sẽ tạo đủ quyền đọc S3 cho crawler). 4) Cấu hình Output (Database, tiền tố bảng, lịch chạy) Output: Target database: chọn edu_logs_db Table name prefix (optional): điền logs_ để bảng tạo ra có tiền tố dễ nhận biết (ví dụ logs_edu_logs_processed) Schedule (lịch chạy): Chọn On demand (chạy tay khi cần) Hoặc chọn Hourly nếu muốn crawler tự chạy mỗi giờ Review → Create để tạo crawler Hình minh họa (sẽ cập nhật)\n5) Chạy Crawler lần đầu Quay lại danh sách Crawlers Chọn crawler-edu-logs-processed → nhấn Run crawler Chờ trạng thái chạy Completed (tuỳ dữ liệu, vài chục giây đến vài phút) Hình minh họa (sẽ cập nhật)\n6) Kiểm tra bảng \u0026amp; schema Vào AWS Glue → Data Catalog → Tables Bạn sẽ thấy bảng mới, ví dụ: Click vào bảng → tab Schema để xem các cột/kiểu dữ liệu có đúng kỳ vọng không Kiểm tra tab Location của bảng trỏ về thư mục: Hình minh họa (sẽ cập nhật)\nMẹo \u0026amp; Lỗi thường gặp Không thấy bảng sau khi chạy\nKiểm tra trong edu-logs-processed-... đã có file Parquet hay chưa\nĐường dẫn Include path có đúng bucket/process prefix không\nChạy lại crawler (Run crawler) sau khi đã có dữ liệu\nSchema không đúng/thiếu cột\nKhi format đầu vào thay đổi, hãy Run crawler lại để cập nhật\nVới partition year/month/day, khi query trên Athena nhớ lọc theo partition để nhanh \u0026amp; rẻ\nCrawler chạy lâu\nSau lần đầu (full scan), đổi Recrawl behavior sang Recrawl changed partitions only\nKết quả mong đợi Có Database: edu_logs_db Có Table (ví dụ): logs_edu_logs_processed trỏ đến S3 processed Sẵn sàng chuyển sang Bước 5: Truy vấn bằng Amazon Athena "
},
{
	"uri": "//localhost:1313/5-athena/",
	"title": "Truy vấn dữ liệu với Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Cấu hình Amazon Athena để truy vấn dữ liệu đã xử lý trong S3 Processed Bucket. Thực hiện các truy vấn SQL mẫu để phân tích dữ liệu log. Cấu hình Partition Projection để tối ưu chi phí và tốc độ truy vấn. Mở Athena và cấu hình kết quả truy vấn\nVào AWS Console → chọn Athena → Query editor. Ở góc trên bên phải, click biểu tượng Settings → Manage. Tại Location of query result, nhập: s3://edu-logs-processed-\u0026lt;account\u0026gt;/athena-results/ Click Save để lưu cấu hình. Chọn Data source và Database\nData source: chọn AwsDataCatalog.\nDatabase: chọn edu_logs_db.\nTrong Tables, chọn bảng logs_edu_logs_processed để kiểm tra schema.\nChạy các truy vấn SQL mẫu\nXem 10 bản ghi đầu tiên:\nSELECT * FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; LIMIT 10; Top hoạt động theo người dùng:\nSELECT user_id, COUNT(*) AS activity_count FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; WHERE year = 2025 AND month = 8 GROUP BY user_id ORDER BY activity_count DESC; Thời lượng học trung bình theo khoá học:\nSELECT course_id, AVG(CAST(duration_sec AS DOUBLE)) AS avg_duration_sec FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed_\u0026lt;account\u0026gt;\u0026#34; WHERE year = 2025 AND month = 8 GROUP BY course_id ORDER BY avg_duration_sec DESC; (Tuỳ chọn) Cấu hình Partition Projection để tối ưu truy vấn\nTrong Athena → Tables → chọn bảng logs_edu_logs_processed → Edit table. Thêm các Parameters sau: projection.enabled = true\rprojection.year.type = integer\rprojection.year.range = 2024,2030\rprojection.month.type = integer\rprojection.month.range = 1,12\rprojection.month.digits = 2\rprojection.day.type = integer\rprojection.day.range = 1,31\rprojection.day.digits = 2\rstorage.location.template = s3://edu-logs-processed-\u0026lt;account\u0026gt;/year=${year}/month=${month}/day=${day}/ Lưu thay đổi. Lưu ý: Khi sử dụng Partition Projection, trong truy vấn bắt buộc lọc theo year, month, day để hạn chế lượng dữ liệu quét và giảm chi phí.\nVí dụ:\nSELECT * FROM \u0026#34;edu_logs_db\u0026#34;.\u0026#34;logs_edu_logs_processed\u0026#34; WHERE year = 2025 AND month = 8 AND day = 8; "
},
{
	"uri": "//localhost:1313/6-cloudwatch/",
	"title": "Giám sát quy trình với Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Trong bước này, chúng ta sẽ:\nGiám sát log của Lambda Function bằng CloudWatch Logs Insights. Tạo Metric Filter để đếm số file được xử lý. Thiết lập SNS + CloudWatch Alarm để nhận cảnh báo khi có lỗi. Kiểm thử quy trình end-to-end. Sử dụng CloudWatch Logs Insights để truy vấn log Lambda Vào AWS Console → CloudWatch → Logs → Logs Insights. Tại Log groups, chọn /aws/lambda/process-edu-logs. Dán truy vấn mẫu sau: fields @timestamp, @message | filter @message like /Processed/ | sort @timestamp desc | limit 50 Chạy query để xem các log gần đây có chứa từ khóa Processed (báo hiệu file đã xử lý thành công). 2. Tạo Metric Filter để đếm số file xử lý Vào CloudWatch → Logs → Log groups → /aws/lambda/process-edu-logs. Chọn Metric filters → Create metric filter. Filter pattern: Processed Metric name: ProcessedFilesCount Namespace: Edu/LogPipeline Value: 1 Sau khi tạo xong, vào CloudWatch → Metrics → Edu/LogPipeline để thấy metric mới. 3. Tạo SNS + CloudWatch Alarm để cảnh báo lỗi Lambda Bước 1: Tạo SNS Topic Vào Simple Notification Service (SNS) → Create topic. Type: Standard Name: edu-pipeline-alerts Create subscription: Protocol: Email Endpoint: Nhập email của bạn. Mở email và Confirm subscription. Bước 2: Tạo CloudWatch Alarm Vào CloudWatch → Alarms → Create alarm. Chọn metric: Lambda → By function name → process-edu-logs → Errors. Điều kiện: \u0026gt;= 3 trong 5 phút. Notification: Chọn topic edu-pipeline-alerts. Create alarm. 4. Kiểm thử quy trình End-to-End Upload thêm file vào: s3://edu-logs-raw-\u0026lt;account\u0026gt;/ingest/\nVào CloudWatch Logs để xem log của Lambda cho lần chạy mới.\nKiểm tra S3 processed bucket có file .parquet trong đúng prefix: year=YYYY/month=MM/day=DD/\nChạy Crawler (nếu không dùng Partition Projection) → Query lại Athena.\nXác nhận:\nMetric ProcessedFilesCount tăng.\nAlarm không đỏ (trừ khi bạn đang test lỗi).\n"
},
{
	"uri": "//localhost:1313/7-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Giải phóng các tài nguyên AWS đã tạo trong workshop để tránh phát sinh chi phí. Xóa các bucket, role, hàm Lambda, và các cấu hình liên quan. 1. Xóa dữ liệu và Bucket S3 Vào AWS S3 Console. Xóa toàn bộ dữ liệu trong: edu-logs-raw-\u0026lt;account\u0026gt; edu-logs-processed-\u0026lt;account\u0026gt; Sau khi trống dữ liệu, chọn Empty và Delete bucket cho cả hai bucket. 2. Xóa Lambda Function Vào AWS Lambda Console. Chọn hàm Lambda process-edu-logs → Actions → Delete. 3. Xóa IAM Role và Policy Vào IAM Console. Chọn Roles → tìm lambda-edu-log-role → Delete role. Vào Policies → tìm EduLogLambdaPolicy → Delete policy. 4. Xóa Glue Crawler và Database Vào AWS Glue Console. Chọn Crawlers → chọn crawler-edu-logs-processed → Delete. Chọn Databases → chọn edu_logs_db → Delete. 5. Xóa Workgroup và Kết quả Athena Vào Amazon Athena → Workgroups → xóa workgroup tùy chỉnh (nếu có). Xóa thư mục athena-results/ trong bucket edu-logs-processed-\u0026lt;account\u0026gt;. 6. Xóa Metric và Alarm trên CloudWatch Vào CloudWatch Console: Alarms → xóa alarm liên quan. Log groups → /aws/lambda/process-edu-logs → Delete log group. Metrics → xóa metric custom trong namespace Edu/LogPipeline. 7. Xóa SNS Topic Vào SNS Console → chọn edu-pipeline-alerts → Delete topic. 8. Xác nhận không còn tài nguyên hoạt động Kiểm tra lại: S3 không còn bucket workshop. Lambda không còn function process-edu-logs. IAM không còn role/policy liên quan. Glue không còn database/crawler. CloudWatch không còn log group, metric, alarm. SNS không còn topic. "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/section-name/",
	"title": "Section-Names",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]